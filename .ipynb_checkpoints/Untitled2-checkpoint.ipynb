{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://graph.facebook.com/v2.12/tenerifevacanze/feed?fields=id,message,reactions,shares,from,caption,created_time,likes.summary(true)\n"
     ]
    }
   ],
   "source": [
    "#! /usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import requests\n",
    "import json\n",
    "import pymongo\n",
    "import pandas\n",
    "import re\n",
    "import nltk\n",
    "import numpy\n",
    "import matplotlib\n",
    "\n",
    "from pymongo import MongoClient\n",
    "from requests_oauthlib import OAuth2\n",
    "from tqdm import tqdm\n",
    "\n",
    "#Base de datos \n",
    "client = MongoClient('localhost:27017')\n",
    "db = client.db_analyzer\n",
    "db_analyzer = db.analyzer\n",
    "collection_posts = db_analyzer.posts\n",
    "collection_comments = db_analyzer.comments\n",
    "\n",
    "id = \"tenerifevacanze\"              # id de la p√°gina que va a ser analizada\n",
    "\"\"\"P√°ginas usadas\n",
    " * tenerifevacanze\n",
    " * VisitTenerifeES\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "url = 'https://graph.facebook.com/v2.12/'\n",
    "page_url = 'https://graph.facebook.com/v2.12/%s/feed?fields=id,message,reactions,shares,from,caption,created_time,likes.summary(true)' %id\n",
    "print(page_url)\n",
    "# Todos los campos deben de ser especificados\n",
    "comments_url = 'https://graph.facebook.com/v2.12/{post_id}/comments?filter=stream&limit=100'\n",
    "\n",
    "# Variable con el token de acceso de Facebook\n",
    "params = {'access_token' : 'EAACQFfa9JeIBAKK0ZCX9x7vGxtDjfDjLLo5W8qVz5REnvqr30DSEy4EgrZAU0RLG0AxYo2UFdfU3hUqtm3T3dEys1eZC1BDq1IRnfpZAKwDDVi6n0LqAYKgOazNbj26FOD7zPGhZCz3RlFw4ADrqIHLC3yc1m7EEZD'}\n",
    "posts = requests.get(page_url, params = params)\n",
    "posts = posts.json()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recopilando datos...\n",
      "Recopilando datos...\n",
      "Recopilando datos...\n",
      "Recopilando datos...\n",
      "Recopilando datos...\n",
      "Recopilando datos...\n",
      "Recopilando datos...\n",
      "Recopilando datos...\n",
      "Recopilando datos...\n",
      "Recopilando datos...\n",
      "Recopilando datos...\n",
      "Recopilando datos...\n",
      "Recopilando datos...\n",
      "Recopilando datos...\n",
      "Recopilando datos...\n",
      "Recopilando datos...\n",
      "Recopilando datos...\n",
      "Recopilando datos...\n",
      "Recopilando datos...\n",
      "Recopilando datos...\n",
      "Recopilando datos...\n",
      "Recopilando datos...\n",
      "Recopilando datos...\n",
      "Recopilando datos...\n",
      "Recopilando datos...\n",
      "Recopilando datos...\n",
      "Recopilando datos...\n",
      "Recopilando datos...\n",
      "Recopilando datos...\n",
      "Recopilando datos...\n",
      "Recopilando datos...\n",
      "Recopilando datos...\n",
      "Recopilando datos...\n",
      "Recopilando datos...\n",
      "Recopilando datos...\n",
      "Recopilando datos...\n",
      "Recopilando datos...\n",
      "Recopilando datos...\n",
      "Recopilando datos...\n",
      "Recopilando datos...\n",
      "'next'\n",
      "Datos recopilados\n"
     ]
    }
   ],
   "source": [
    "# Data extraction\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        print(\"Recopilando datos...\")\n",
    "        ### Recupera un post\n",
    "        for element in posts['data']:\n",
    "            collection_posts.insert_one(element)\n",
    "            #### Recupera todos los comentarios de ese post\n",
    "            this_comment_url = comments_url.replace(\"{post_id}\", element['id'])\n",
    "            comments = requests.get(this_comment_url, params = params).json()\n",
    "            # Recorre todos los comentarios hasta que la respuesta est√© vac√≠a, que no haya m√°s comentarios\n",
    "            while ('paging' in comments and 'cursors' in comments['paging'] and 'after' in comments['paging']['cursors']):\n",
    "                ### Itera a trav√©s de todos los comentarios\n",
    "                for comment in comments['data']:\n",
    "                    comment['post_id'] = element['id']\n",
    "                    collection_comments.insert_one(comment)\n",
    "                comments = requests.get(this_comment_url + '&after=' + comments['paging']['cursors']['after'], params = params).json()\n",
    "        #### Vamos a la siguiente p√°gina en feed\n",
    "        posts = requests.get(posts['paging']['next']).json()\n",
    "    except KeyError as e:\n",
    "        print(e)\n",
    "        break\n",
    "\n",
    "print(\"Datos recopilados\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33755\n",
      "449866\n"
     ]
    }
   ],
   "source": [
    "# Data pull\n",
    "\n",
    "posts_data = []\n",
    "comments_data = []\n",
    "\n",
    "for doc in collection_posts.find({}):\n",
    "    if 'message' in doc.keys():\n",
    "        posts_data.append((doc['message'], doc['created_time'], doc['likes']['summary']['total_count'], doc.get('shares', {'count': 0})['count'], doc['id']))\n",
    "    else:\n",
    "        pass\n",
    "        #print(\"No message\")\n",
    "        \n",
    "print(len(posts_data))\n",
    "    \n",
    "for comment in collection_comments.find({}):\n",
    "    if 'message' in comment.keys():\n",
    "        comments_data.append((comment['message'], comment['created_time'], comment['post_id']))\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "print(len(comments_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  message  \\\n",
      "0                               Google srteet view t√ºrkye   \n",
      "1       Lets hope it never makes it to America, no one...   \n",
      "2       America wants to destroy Europe through the Eu...   \n",
      "3       google shot up because you have enough of goog...   \n",
      "4                              goodotgl websit is offline   \n",
      "5                                                  follow   \n",
      "6                                           i hate google   \n",
      "7                                           i hate google   \n",
      "8                                 apple is so much better   \n",
      "9                                    chromecraps? really?   \n",
      "10      I Love ‚ù§Ô∏è Google as hate only gets you more ha...   \n",
      "11                                                  video   \n",
      "12                                          coment sa va?   \n",
      "13                                                     OK   \n",
      "14                                                          \n",
      "15                                                          \n",
      "16      Google translator make to laugh many times !\\n...   \n",
      "17      hello Youtube, pls i have video's that are mon...   \n",
      "18      Hello friends Dinamarca. Happy Good Week....Go...   \n",
      "19      Google, please bring back the \"View\" button in...   \n",
      "20      Google aka Gulag has silenced pro military and...   \n",
      "21      Google, please bring back the \"View\" function ...   \n",
      "22      Define Arkeoloji ve Tarih Seven Herkesi Sayfam...   \n",
      "23      Hey Google, I am trying ever so hard to log in...   \n",
      "24                                              Sany lion   \n",
      "25               Why say ex army people and not veterans?   \n",
      "26      \"Businesses that make money by collecting and ...   \n",
      "27      ŸÑŸÜÿ¨ÿπŸÑ ŸäŸàŸÖ 21 ŸÖÿßÿ±ÿ≥ ŸÖŸÜ ŸÉŸÑ ÿπÿßŸÖ ÿπŸäÿØÿß ŸÑŸÑÿ•ŸÜÿ≥ÿßŸÜŸäÿ© ÿ¨ŸÖÿπ...   \n",
      "28      Simon Klint Bergh her kan du se, hvad der kom ...   \n",
      "29                                            Sexy movied   \n",
      "...                                                   ...   \n",
      "449836  mi vorrei stare adesso liiiiiiiiiiiiiiiiiiiiii...   \n",
      "449837  io ci sono stato per due anni....bello davvero...   \n",
      "449838                         playa della teresitassssss   \n",
      "449839      quand un volcan s eteint un etre s'√©veille!!!   \n",
      "449840                                        Linda isla!   \n",
      "449841                           Q BELLEZA de lugar.‚ô• :-)   \n",
      "449842  anche io ci sono stato per due anni e il pross...   \n",
      "449843  Davide De Rosa Francesco D'Ambrose Stefano Mol...   \n",
      "449844                                   Dominik Limmer üòç   \n",
      "449845                 Looks so beautiful. Wud luv to see   \n",
      "449846  me encantaria vivirlo en directo alguna vez en...   \n",
      "449847  Si no vieron el documental de TVE 2 sobre el C...   \n",
      "449848            en diciembre  meme esperais en tenerife   \n",
      "449849  io l'ho visto e vissuto 2 volte.....sempliceme...   \n",
      "449850  si santa cruz √® bella per√≤ sta a nord ed √® pi√π...   \n",
      "449851  Es la isla de los encantos! En ella pase los m...   \n",
      "449852  En el norte no se, pero si se de muy buen piso...   \n",
      "449853                ooo... Tenerife I MISS YOU!!!!!!!!!   \n",
      "449854                    i miss you.......tenerife !!!!!   \n",
      "449855  Hay muchos hermosos recuerdos. Pronto otra vez!!!   \n",
      "449856     ciao,la prima settimana di febbraio e libero??   \n",
      "449857     Plase open photo and PUSH direct on photo LIKE   \n",
      "449858                   Me gustar√≠a que otra vez pronto!   \n",
      "449859          http://www.facebook.com/borraccinofilippo   \n",
      "449860  Hello , I'm the admon , but I can't write a me...   \n",
      "449861  toda ella es unamaravilla esta preciosa isla d...   \n",
      "449862      in  six weeks we are going to tenerife.:)))))   \n",
      "449863                    cheers for sharing these images   \n",
      "449864                                   mei  piace mucio   \n",
      "449865  Hi Franko! are u already in tenerife? Did you ...   \n",
      "\n",
      "                    created_time                          post_id  \n",
      "0       2018-03-16T20:53:06+0000   104958162837_10156226512267838  \n",
      "1       2018-03-16T20:55:31+0000   104958162837_10156226512267838  \n",
      "2       2018-03-16T21:18:41+0000   104958162837_10156226512267838  \n",
      "3       2018-03-16T21:40:26+0000   104958162837_10156226512267838  \n",
      "4       2018-03-16T21:47:25+0000   104958162837_10156226512267838  \n",
      "5       2018-03-16T23:50:25+0000   104958162837_10156226512267838  \n",
      "6       2018-03-17T01:35:09+0000   104958162837_10156226512267838  \n",
      "7       2018-03-17T01:35:17+0000   104958162837_10156226512267838  \n",
      "8       2018-03-17T01:35:29+0000   104958162837_10156226512267838  \n",
      "9       2018-03-17T01:35:45+0000   104958162837_10156226512267838  \n",
      "10      2018-03-17T02:02:19+0000   104958162837_10156226512267838  \n",
      "11      2018-03-17T09:32:27+0000   104958162837_10156226512267838  \n",
      "12      2018-03-17T09:33:35+0000   104958162837_10156226512267838  \n",
      "13      2018-03-17T13:05:54+0000   104958162837_10156226512267838  \n",
      "14      2018-03-17T14:24:55+0000   104958162837_10156226512267838  \n",
      "15      2018-03-17T14:25:20+0000   104958162837_10156226512267838  \n",
      "16      2018-03-17T19:43:45+0000   104958162837_10156226512267838  \n",
      "17      2018-03-18T09:10:56+0000   104958162837_10156226512267838  \n",
      "18      2018-03-18T13:46:48+0000   104958162837_10156226512267838  \n",
      "19      2018-03-18T14:31:01+0000   104958162837_10156226512267838  \n",
      "20      2018-03-18T15:27:38+0000   104958162837_10156226512267838  \n",
      "21      2018-03-18T19:25:22+0000   104958162837_10156226512267838  \n",
      "22      2018-03-19T02:02:29+0000   104958162837_10156226512267838  \n",
      "23      2018-03-19T02:07:04+0000   104958162837_10156226512267838  \n",
      "24      2018-03-19T02:11:50+0000   104958162837_10156226512267838  \n",
      "25      2018-03-19T03:59:44+0000   104958162837_10156226512267838  \n",
      "26      2018-03-19T04:00:07+0000   104958162837_10156226512267838  \n",
      "27      2018-03-19T08:28:14+0000   104958162837_10156226512267838  \n",
      "28      2018-03-19T10:21:24+0000   104958162837_10156226512267838  \n",
      "29      2018-03-19T11:05:33+0000   104958162837_10156226512267838  \n",
      "...                          ...                              ...  \n",
      "449836  2012-04-29T13:07:25+0000  145257262182078_339113562796446  \n",
      "449837  2012-05-24T15:22:09+0000  145257262182078_339113562796446  \n",
      "449838  2012-05-28T15:18:43+0000  145257262182078_339113562796446  \n",
      "449839  2013-02-27T17:13:56+0000  145257262182078_339113562796446  \n",
      "449840  2013-11-15T22:08:24+0000  145257262182078_339113562796446  \n",
      "449841  2014-07-02T22:32:15+0000  145257262182078_339113562796446  \n",
      "449842  2015-09-10T13:27:59+0000  145257262182078_339113562796446  \n",
      "449843  2015-11-03T12:41:30+0000  145257262182078_339113562796446  \n",
      "449844  2017-05-04T19:00:59+0000  145257262182078_339113562796446  \n",
      "449845  2018-03-23T13:51:37+0000  145257262182078_339113562796446  \n",
      "449846  2012-02-24T18:43:38+0000  145257262182078_348655938508189  \n",
      "449847  2012-03-15T18:29:03+0000  145257262182078_348655938508189  \n",
      "449848  2013-09-26T18:29:55+0000  145257262182078_348655938508189  \n",
      "449849  2012-01-27T19:57:50+0000  145257262182078_317800838261052  \n",
      "449850  2016-04-05T19:59:36+0000  145257262182078_317800838261052  \n",
      "449851  2012-02-02T06:26:08+0000  145257262182078_243856372355958  \n",
      "449852  2012-02-02T06:30:09+0000  145257262182078_308750549166081  \n",
      "449853  2011-12-27T18:42:41+0000  145257262182078_297437646966251  \n",
      "449854  2011-12-27T19:06:53+0000  145257262182078_297437646966251  \n",
      "449855  2011-12-27T19:20:18+0000  145257262182078_297437646966251  \n",
      "449856  2011-12-25T18:07:05+0000  145257262182078_290074864366983  \n",
      "449857  2011-12-11T17:01:05+0000  145257262182078_286604488042635  \n",
      "449858  2011-11-19T20:05:38+0000  145257262182078_130910117018677  \n",
      "449859  2011-12-05T15:05:44+0000  145257262182078_130910117018677  \n",
      "449860  2011-12-05T15:17:54+0000  145257262182078_130910117018677  \n",
      "449861  2011-10-01T18:53:25+0000  145257262182078_264512243589912  \n",
      "449862  2011-05-22T10:34:01+0000  145257262182078_264512193589917  \n",
      "449863  2010-11-16T10:25:26+0000  145257262182078_159417384099399  \n",
      "449864  2013-12-24T15:19:36+0000  145257262182078_159417384099399  \n",
      "449865  2011-03-05T22:54:12+0000  145257262182078_159207717453699  \n",
      "\n",
      "[449866 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "df_posts = pandas.DataFrame(posts_data)\n",
    "df_posts.columns = ['message', 'created_time', 'likes', 'shares', 'post_id']\n",
    "df_comments = pandas.DataFrame(comments_data)\n",
    "df_comments.columns = ['message', 'created_time', 'post_id']\n",
    "print(df_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature extraction\n",
    "\n",
    "\n",
    "# Funciones\n",
    "\n",
    "# Limpia comentarios y posts\n",
    "def preprocess(text):\n",
    "    # Limpieza b√°sica\n",
    "    # Limpia de espacios y signos de puntuaci√≥n, y convierte en min√∫scula\n",
    "    text = text.strip()\n",
    "    text = re.sub(r'[^\\w\\s]','',text)\n",
    "    text = text.lower()\n",
    "\n",
    "    # Divide en tokens\n",
    "    tokens = nltk.word_tokenize(text) \n",
    "\n",
    "    return(tokens,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtiene los hastags de los mensajes\n",
    "def get_hashtags(text):\n",
    "    hashtags = re.findall(r\"#(\\w+)\", text)\n",
    "    return(hashtags,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "def tag_tokens(preprocessed_tokens):\n",
    "    pos = nltk.pos_tag(preprocessed_tokens)\n",
    "    return(pos,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "def get_keywords(tagged_tokens, pos='all'):\n",
    "\n",
    "    if(pos == 'all'):\n",
    "        lst_pos = ('NN','JJ','VB')\n",
    "    elif(pos == 'nouns'):\n",
    "        lst_pos = 'NN'\n",
    "    elif(pos == 'verbs'):\n",
    "        lst_pos = 'VB'\n",
    "    elif(pos == 'adjectives'):\n",
    "        lst_pos = 'JJ'\n",
    "    else:\n",
    "        lst_pos = ('NN','JJ','VB')\n",
    "\n",
    "    keywords = [tup[0] for tup in tagged_tokens if tup[1].startswith(lst_pos)]\n",
    "    return(keywords,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_noun_phrases(tagged_tokens):\n",
    "\n",
    "    grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
    "    cp = nltk.RegexpParser(grammar)\n",
    "    tree = cp.parse(tagged_tokens)\n",
    "\n",
    "    result = []\n",
    "    for subtree in tree.subtrees(filter=lambda t: t.label() == 'NP'):\n",
    "        #Solo tomamos frases, no palabras sueltas\n",
    "        if(len(subtree.leaves()) > 1):\n",
    "            outputs = [tup[0] for tup in subtree.leaves()]\n",
    "            outputs = \" \".join(outputs)\n",
    "            result.append(outputs)\n",
    "    return(result,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_pipeline(dataframe):\n",
    "    print(\"Entro execute pipeline\")\n",
    "    #Obtener hashtags\n",
    "    dataframe['hashtags'] = dataframe.apply(lambda x: get_hashtags(x['message']), axis=1)\n",
    "    print(\"Paso 1\")\n",
    "    #\n",
    "    dataframe['preprocessed'] = dataframe.apply(lambda x: preprocess(x['message'][0]), axis=1, reduce=True)\n",
    "    print(\"Paso 2\")\n",
    "    #\n",
    "    dataframe['tagged'] = dataframe.apply(lambda x: tag_tokens(x['preprocessed'][0]), axis=1)\n",
    "    print(\"Paso 3\") \n",
    "    #Extraer palabras clave\n",
    "    dataframe['keywords'] = dataframe.apply(lambda x: get_keywords(x['tagged'][0], 'all'), axis=1)\n",
    "    print(\"Paso 4\")\n",
    "    #\n",
    "    dataframe['noun_phrases'] = dataframe.apply(lambda x: get_noun_phrases(x['tagged'][0]), axis=1)\n",
    "    print(\"Paso 5\")\n",
    "    print(\"Salgo execute pipeline\")\n",
    "    return(dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entro execute pipeline\n",
      "Paso 1\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Wrong number of items passed 6, placement implies 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32md:\\python\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2524\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2525\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2526\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'preprocessed'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32md:\\python\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36mset\u001b[1;34m(self, item, value, check)\u001b[0m\n\u001b[0;32m   3967\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3968\u001b[1;33m             \u001b[0mloc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3969\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2526\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2527\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2528\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'preprocessed'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-00c554ee0bbd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf_posts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexecute_pipeline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_posts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-11-f2a21e66e89e>\u001b[0m in \u001b[0;36mexecute_pipeline\u001b[1;34m(dataframe)\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Paso 1\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mdataframe\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'preprocessed'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataframe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'message'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Paso 2\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   2517\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2518\u001b[0m             \u001b[1;31m# set column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2519\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2520\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2521\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_setitem_slice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_set_item\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   2584\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ensure_valid_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2585\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sanitize_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2586\u001b[1;33m         \u001b[0mNDFrame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2587\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2588\u001b[0m         \u001b[1;31m# check if we are modifying a copy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_set_item\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   1952\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1953\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_set_item\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1954\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1955\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_clear_item_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1956\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36mset\u001b[1;34m(self, item, value, check)\u001b[0m\n\u001b[0;32m   3969\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3970\u001b[0m             \u001b[1;31m# This item wasn't present, just insert at end\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3971\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3972\u001b[0m             \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3973\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36minsert\u001b[1;34m(self, loc, item, value, allow_duplicates)\u001b[0m\n\u001b[0;32m   4070\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4071\u001b[0m         block = make_block(values=value, ndim=self.ndim,\n\u001b[1;32m-> 4072\u001b[1;33m                            placement=slice(loc, loc + 1))\n\u001b[0m\u001b[0;32m   4073\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4074\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mblkno\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcount\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_fast_count_smallints\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_blknos\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36mmake_block\u001b[1;34m(values, placement, klass, ndim, dtype, fastpath)\u001b[0m\n\u001b[0;32m   2955\u001b[0m                      placement=placement, dtype=dtype)\n\u001b[0;32m   2956\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2957\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mklass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mndim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfastpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfastpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplacement\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mplacement\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2958\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2959\u001b[0m \u001b[1;31m# TODO: flexible with index=None and/or items=None\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, values, ndim, fastpath, placement, **kwargs)\u001b[0m\n\u001b[0;32m   2080\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2081\u001b[0m         super(ObjectBlock, self).__init__(values, ndim=ndim, fastpath=fastpath,\n\u001b[1;32m-> 2082\u001b[1;33m                                           placement=placement, **kwargs)\n\u001b[0m\u001b[0;32m   2083\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2084\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, values, placement, ndim, fastpath)\u001b[0m\n\u001b[0;32m    118\u001b[0m             raise ValueError('Wrong number of items passed %d, placement '\n\u001b[0;32m    119\u001b[0m                              'implies %d' % (len(self.values),\n\u001b[1;32m--> 120\u001b[1;33m                                              len(self.mgr_locs)))\n\u001b[0m\u001b[0;32m    121\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Wrong number of items passed 6, placement implies 1"
     ]
    }
   ],
   "source": [
    "df_posts = execute_pipeline(df_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
