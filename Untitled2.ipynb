{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://graph.facebook.com/v2.12/tenerifevacanze/feed?fields=id,message,reactions,shares,from,caption,created_time,likes.summary(true)\n"
     ]
    }
   ],
   "source": [
    "#! /usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import requests\n",
    "import json\n",
    "import pymongo\n",
    "import pandas\n",
    "import re\n",
    "import nltk\n",
    "import numpy\n",
    "import matplotlib\n",
    "\n",
    "from pymongo import MongoClient\n",
    "from requests_oauthlib import OAuth2\n",
    "from tqdm import tqdm\n",
    "\n",
    "#Base de datos \n",
    "client = MongoClient('localhost:27017')\n",
    "db = client.db_analyzer\n",
    "db_analyzer = db.analyzer\n",
    "collection_posts = db_analyzer.posts\n",
    "collection_comments = db_analyzer.comments\n",
    "\n",
    "id = \"tenerifevacanze\"              # id de la p√°gina que va a ser analizada\n",
    "\"\"\"P√°ginas usadas\n",
    " * tenerifevacanze\n",
    " * VisitTenerifeES\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "url = 'https://graph.facebook.com/v2.12/'\n",
    "page_url = 'https://graph.facebook.com/v2.12/%s/feed?fields=id,message,reactions,shares,from,caption,created_time,likes.summary(true)' %id\n",
    "print(page_url)\n",
    "# Todos los campos deben de ser especificados\n",
    "comments_url = 'https://graph.facebook.com/v2.12/{post_id}/comments?filter=stream&limit=100'\n",
    "\n",
    "# Variable con el token de acceso de Facebook\n",
    "params = {'access_token' : 'EAACQFfa9JeIBAKK0ZCX9x7vGxtDjfDjLLo5W8qVz5REnvqr30DSEy4EgrZAU0RLG0AxYo2UFdfU3hUqtm3T3dEys1eZC1BDq1IRnfpZAKwDDVi6n0LqAYKgOazNbj26FOD7zPGhZCz3RlFw4ADrqIHLC3yc1m7EEZD'}\n",
    "posts = requests.get(page_url, params = params)\n",
    "posts = posts.json()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recopilando datos...\n",
      "Recopilando datos...\n",
      "Recopilando datos...\n",
      "Recopilando datos...\n",
      "Recopilando datos...\n",
      "Recopilando datos...\n",
      "Recopilando datos...\n",
      "Recopilando datos...\n",
      "Recopilando datos...\n",
      "Recopilando datos...\n",
      "Recopilando datos...\n",
      "Recopilando datos...\n",
      "Recopilando datos...\n",
      "Recopilando datos...\n",
      "Recopilando datos...\n",
      "Recopilando datos...\n",
      "Recopilando datos...\n",
      "Recopilando datos...\n",
      "Recopilando datos...\n",
      "Recopilando datos...\n",
      "Recopilando datos...\n",
      "Recopilando datos...\n",
      "Recopilando datos...\n",
      "Recopilando datos...\n",
      "Recopilando datos...\n",
      "Recopilando datos...\n",
      "Recopilando datos...\n",
      "Recopilando datos...\n",
      "Recopilando datos...\n",
      "Recopilando datos...\n",
      "Recopilando datos...\n",
      "Recopilando datos...\n",
      "Recopilando datos...\n",
      "Recopilando datos...\n",
      "Recopilando datos...\n",
      "Recopilando datos...\n",
      "Recopilando datos...\n",
      "Recopilando datos...\n",
      "Recopilando datos...\n",
      "Recopilando datos...\n",
      "'next'\n",
      "Datos recopilados\n"
     ]
    }
   ],
   "source": [
    "# Data extraction\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        print(\"Recopilando datos...\")\n",
    "        ### Recupera un post\n",
    "        for element in posts['data']:\n",
    "            collection_posts.insert_one(element)\n",
    "            #### Recupera todos los comentarios de ese post\n",
    "            this_comment_url = comments_url.replace(\"{post_id}\", element['id'])\n",
    "            comments = requests.get(this_comment_url, params = params).json()\n",
    "            # Recorre todos los comentarios hasta que la respuesta est√© vac√≠a, que no haya m√°s comentarios\n",
    "            while ('paging' in comments and 'cursors' in comments['paging'] and 'after' in comments['paging']['cursors']):\n",
    "                ### Itera a trav√©s de todos los comentarios\n",
    "                for comment in comments['data']:\n",
    "                    comment['post_id'] = element['id']\n",
    "                    collection_comments.insert_one(comment)\n",
    "                comments = requests.get(this_comment_url + '&after=' + comments['paging']['cursors']['after'], params = params).json()\n",
    "        #### Vamos a la siguiente p√°gina en feed\n",
    "        posts = requests.get(posts['paging']['next']).json()\n",
    "    except KeyError as e:\n",
    "        print(e)\n",
    "        break\n",
    "\n",
    "print(\"Datos recopilados\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34571\n",
      "456097\n"
     ]
    }
   ],
   "source": [
    "# Data pull\n",
    "\n",
    "posts_data = []\n",
    "comments_data = []\n",
    "\n",
    "for doc in collection_posts.find({}):\n",
    "    if 'message' in doc.keys():\n",
    "        posts_data.append((doc['message'], doc['created_time'], doc['likes']['summary']['total_count'], doc.get('shares', {'count': 0})['count'], doc['id']))\n",
    "    else:\n",
    "        pass\n",
    "        #print(\"No message\")\n",
    "        \n",
    "print(len(posts_data))\n",
    "    \n",
    "for comment in collection_comments.find({}):\n",
    "    if 'message' in comment.keys():\n",
    "        comments_data.append((comment['message'], comment['created_time'], comment['post_id']))\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "print(len(comments_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  message  \\\n",
      "0                               Google srteet view t√ºrkye   \n",
      "1       Lets hope it never makes it to America, no one...   \n",
      "2       America wants to destroy Europe through the Eu...   \n",
      "3       google shot up because you have enough of goog...   \n",
      "4                              goodotgl websit is offline   \n",
      "5                                                  follow   \n",
      "6                                           i hate google   \n",
      "7                                           i hate google   \n",
      "8                                 apple is so much better   \n",
      "9                                    chromecraps? really?   \n",
      "10      I Love ‚ù§Ô∏è Google as hate only gets you more ha...   \n",
      "11                                                  video   \n",
      "12                                          coment sa va?   \n",
      "13                                                     OK   \n",
      "14                                                          \n",
      "15                                                          \n",
      "16      Google translator make to laugh many times !\\n...   \n",
      "17      hello Youtube, pls i have video's that are mon...   \n",
      "18      Hello friends Dinamarca. Happy Good Week....Go...   \n",
      "19      Google, please bring back the \"View\" button in...   \n",
      "20      Google aka Gulag has silenced pro military and...   \n",
      "21      Google, please bring back the \"View\" function ...   \n",
      "22      Define Arkeoloji ve Tarih Seven Herkesi Sayfam...   \n",
      "23      Hey Google, I am trying ever so hard to log in...   \n",
      "24                                              Sany lion   \n",
      "25               Why say ex army people and not veterans?   \n",
      "26      \"Businesses that make money by collecting and ...   \n",
      "27      ŸÑŸÜÿ¨ÿπŸÑ ŸäŸàŸÖ 21 ŸÖÿßÿ±ÿ≥ ŸÖŸÜ ŸÉŸÑ ÿπÿßŸÖ ÿπŸäÿØÿß ŸÑŸÑÿ•ŸÜÿ≥ÿßŸÜŸäÿ© ÿ¨ŸÖÿπ...   \n",
      "28      Simon Klint Bergh her kan du se, hvad der kom ...   \n",
      "29                                            Sexy movied   \n",
      "...                                                   ...   \n",
      "456067  mi vorrei stare adesso liiiiiiiiiiiiiiiiiiiiii...   \n",
      "456068  io ci sono stato per due anni....bello davvero...   \n",
      "456069                         playa della teresitassssss   \n",
      "456070      quand un volcan s eteint un etre s'√©veille!!!   \n",
      "456071                                        Linda isla!   \n",
      "456072                           Q BELLEZA de lugar.‚ô• :-)   \n",
      "456073  anche io ci sono stato per due anni e il pross...   \n",
      "456074  Davide De Rosa Francesco D'Ambrose Stefano Mol...   \n",
      "456075                                   Dominik Limmer üòç   \n",
      "456076                 Looks so beautiful. Wud luv to see   \n",
      "456077  me encantaria vivirlo en directo alguna vez en...   \n",
      "456078  Si no vieron el documental de TVE 2 sobre el C...   \n",
      "456079            en diciembre  meme esperais en tenerife   \n",
      "456080  io l'ho visto e vissuto 2 volte.....sempliceme...   \n",
      "456081  si santa cruz √® bella per√≤ sta a nord ed √® pi√π...   \n",
      "456082  Es la isla de los encantos! En ella pase los m...   \n",
      "456083  En el norte no se, pero si se de muy buen piso...   \n",
      "456084                ooo... Tenerife I MISS YOU!!!!!!!!!   \n",
      "456085                    i miss you.......tenerife !!!!!   \n",
      "456086  Hay muchos hermosos recuerdos. Pronto otra vez!!!   \n",
      "456087     ciao,la prima settimana di febbraio e libero??   \n",
      "456088     Plase open photo and PUSH direct on photo LIKE   \n",
      "456089                   Me gustar√≠a que otra vez pronto!   \n",
      "456090          http://www.facebook.com/borraccinofilippo   \n",
      "456091  Hello , I'm the admon , but I can't write a me...   \n",
      "456092  toda ella es unamaravilla esta preciosa isla d...   \n",
      "456093      in  six weeks we are going to tenerife.:)))))   \n",
      "456094                    cheers for sharing these images   \n",
      "456095                                   mei  piace mucio   \n",
      "456096  Hi Franko! are u already in tenerife? Did you ...   \n",
      "\n",
      "                    created_time                          post_id  \n",
      "0       2018-03-16T20:53:06+0000   104958162837_10156226512267838  \n",
      "1       2018-03-16T20:55:31+0000   104958162837_10156226512267838  \n",
      "2       2018-03-16T21:18:41+0000   104958162837_10156226512267838  \n",
      "3       2018-03-16T21:40:26+0000   104958162837_10156226512267838  \n",
      "4       2018-03-16T21:47:25+0000   104958162837_10156226512267838  \n",
      "5       2018-03-16T23:50:25+0000   104958162837_10156226512267838  \n",
      "6       2018-03-17T01:35:09+0000   104958162837_10156226512267838  \n",
      "7       2018-03-17T01:35:17+0000   104958162837_10156226512267838  \n",
      "8       2018-03-17T01:35:29+0000   104958162837_10156226512267838  \n",
      "9       2018-03-17T01:35:45+0000   104958162837_10156226512267838  \n",
      "10      2018-03-17T02:02:19+0000   104958162837_10156226512267838  \n",
      "11      2018-03-17T09:32:27+0000   104958162837_10156226512267838  \n",
      "12      2018-03-17T09:33:35+0000   104958162837_10156226512267838  \n",
      "13      2018-03-17T13:05:54+0000   104958162837_10156226512267838  \n",
      "14      2018-03-17T14:24:55+0000   104958162837_10156226512267838  \n",
      "15      2018-03-17T14:25:20+0000   104958162837_10156226512267838  \n",
      "16      2018-03-17T19:43:45+0000   104958162837_10156226512267838  \n",
      "17      2018-03-18T09:10:56+0000   104958162837_10156226512267838  \n",
      "18      2018-03-18T13:46:48+0000   104958162837_10156226512267838  \n",
      "19      2018-03-18T14:31:01+0000   104958162837_10156226512267838  \n",
      "20      2018-03-18T15:27:38+0000   104958162837_10156226512267838  \n",
      "21      2018-03-18T19:25:22+0000   104958162837_10156226512267838  \n",
      "22      2018-03-19T02:02:29+0000   104958162837_10156226512267838  \n",
      "23      2018-03-19T02:07:04+0000   104958162837_10156226512267838  \n",
      "24      2018-03-19T02:11:50+0000   104958162837_10156226512267838  \n",
      "25      2018-03-19T03:59:44+0000   104958162837_10156226512267838  \n",
      "26      2018-03-19T04:00:07+0000   104958162837_10156226512267838  \n",
      "27      2018-03-19T08:28:14+0000   104958162837_10156226512267838  \n",
      "28      2018-03-19T10:21:24+0000   104958162837_10156226512267838  \n",
      "29      2018-03-19T11:05:33+0000   104958162837_10156226512267838  \n",
      "...                          ...                              ...  \n",
      "456067  2012-04-29T13:07:25+0000  145257262182078_339113562796446  \n",
      "456068  2012-05-24T15:22:09+0000  145257262182078_339113562796446  \n",
      "456069  2012-05-28T15:18:43+0000  145257262182078_339113562796446  \n",
      "456070  2013-02-27T17:13:56+0000  145257262182078_339113562796446  \n",
      "456071  2013-11-15T22:08:24+0000  145257262182078_339113562796446  \n",
      "456072  2014-07-02T22:32:15+0000  145257262182078_339113562796446  \n",
      "456073  2015-09-10T13:27:59+0000  145257262182078_339113562796446  \n",
      "456074  2015-11-03T12:41:30+0000  145257262182078_339113562796446  \n",
      "456075  2017-05-04T19:00:59+0000  145257262182078_339113562796446  \n",
      "456076  2018-03-23T13:51:37+0000  145257262182078_339113562796446  \n",
      "456077  2012-02-24T18:43:38+0000  145257262182078_348655938508189  \n",
      "456078  2012-03-15T18:29:03+0000  145257262182078_348655938508189  \n",
      "456079  2013-09-26T18:29:55+0000  145257262182078_348655938508189  \n",
      "456080  2012-01-27T19:57:50+0000  145257262182078_317800838261052  \n",
      "456081  2016-04-05T19:59:36+0000  145257262182078_317800838261052  \n",
      "456082  2012-02-02T06:26:08+0000  145257262182078_243856372355958  \n",
      "456083  2012-02-02T06:30:09+0000  145257262182078_308750549166081  \n",
      "456084  2011-12-27T18:42:41+0000  145257262182078_297437646966251  \n",
      "456085  2011-12-27T19:06:53+0000  145257262182078_297437646966251  \n",
      "456086  2011-12-27T19:20:18+0000  145257262182078_297437646966251  \n",
      "456087  2011-12-25T18:07:05+0000  145257262182078_290074864366983  \n",
      "456088  2011-12-11T17:01:05+0000  145257262182078_286604488042635  \n",
      "456089  2011-11-19T20:05:38+0000  145257262182078_130910117018677  \n",
      "456090  2011-12-05T15:05:44+0000  145257262182078_130910117018677  \n",
      "456091  2011-12-05T15:17:54+0000  145257262182078_130910117018677  \n",
      "456092  2011-10-01T18:53:25+0000  145257262182078_264512243589912  \n",
      "456093  2011-05-22T10:34:01+0000  145257262182078_264512193589917  \n",
      "456094  2010-11-16T10:25:26+0000  145257262182078_159417384099399  \n",
      "456095  2013-12-24T15:19:36+0000  145257262182078_159417384099399  \n",
      "456096  2011-03-05T22:54:12+0000  145257262182078_159207717453699  \n",
      "\n",
      "[456097 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "df_posts = pandas.DataFrame(posts_data)\n",
    "df_posts.columns = ['message', 'created_time', 'likes', 'shares', 'post_id']\n",
    "df_comments = pandas.DataFrame(comments_data)\n",
    "df_comments.columns = ['message', 'created_time', 'post_id']\n",
    "print(df_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature extraction\n",
    "\n",
    "\n",
    "# Funciones\n",
    "\n",
    "# Limpia comentarios y posts\n",
    "def preprocess(text):\n",
    "    # Limpieza b√°sica\n",
    "    # Limpia de espacios y signos de puntuaci√≥n, y convierte en min√∫scula\n",
    "    text = text.strip()\n",
    "    text = re.sub(r'[^\\w\\s]','',text)\n",
    "    text = text.lower()\n",
    "\n",
    "    # Divide en tokens\n",
    "    tokens = nltk.word_tokenize(text) \n",
    "\n",
    "    return(tokens,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtiene los hastags de los mensajes\n",
    "def get_hashtags(text):\n",
    "    hashtags = re.findall(r\"#(\\w+)\", text)\n",
    "    return(hashtags,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "def tag_tokens(preprocessed_tokens):\n",
    "    pos = nltk.pos_tag(preprocessed_tokens)\n",
    "    return(pos,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "def get_keywords(tagged_tokens, pos='all'):\n",
    "\n",
    "    if(pos == 'all'):\n",
    "        lst_pos = ('NN','JJ','VB')\n",
    "    elif(pos == 'nouns'):\n",
    "        lst_pos = 'NN'\n",
    "    elif(pos == 'verbs'):\n",
    "        lst_pos = 'VB'\n",
    "    elif(pos == 'adjectives'):\n",
    "        lst_pos = 'JJ'\n",
    "    else:\n",
    "        lst_pos = ('NN','JJ','VB')\n",
    "\n",
    "    keywords = [tup[0] for tup in tqdm(tagged_tokens) if tup[1].startswith(lst_pos)]\n",
    "    return(keywords,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_noun_phrases(tagged_tokens):\n",
    "\n",
    "    grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
    "    cp = nltk.RegexpParser(grammar)\n",
    "    tree = cp.parse(tagged_tokens)\n",
    "\n",
    "    result = []\n",
    "    for subtree in tree.subtrees(filter=lambda t: t.label() == 'NP'):\n",
    "        #Solo tomamos frases, no palabras sueltas\n",
    "        if(len(subtree.leaves()) > 1):\n",
    "            outputs = [tup[0] for tup in subtree.leaves()]\n",
    "            outputs = \" \".join(outputs)\n",
    "            result.append(outputs)\n",
    "    return(result,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_pipeline(dataframe):\n",
    "    print(\"Entro execute pipeline\")\n",
    "    #Obtener hashtags\n",
    "    dataframe['hashtags'] = dataframe.apply(lambda x: get_hashtags(x['message']), axis=1)\n",
    "    print(\"Paso 1\")\n",
    "    #\n",
    "    dataframe['preprocessed'] = dataframe.apply(lambda x: preprocess(x['message']), axis=1, reduce=True)\n",
    "    print(\"Paso 2\")\n",
    "    #\n",
    "    dataframe['tagged'] = dataframe.apply(lambda x: tag_tokens(x['preprocessed'][0]), axis=1)\n",
    "    print(\"Paso 3\") \n",
    "    #Extraer palabras clave\n",
    "    dataframe['keywords'] = dataframe.apply(lambda x: get_keywords(x['tagged'][0], 'all'), axis=1)\n",
    "    print(\"Paso 4\")\n",
    "    #\n",
    "    dataframe['noun_phrases'] = dataframe.apply(lambda x: get_noun_phrases(x['tagged'][0]), axis=1)\n",
    "    print(\"Paso 5\")\n",
    "    print(\"Salgo execute pipeline\")\n",
    "    return(dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entro execute pipeline\n",
      "Paso 1\n",
      "Paso 2\n",
      "Paso 3\n",
      "Paso 4\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Paso 5\n",
      "Salgo execute pipeline\n"
     ]
    }
   ],
   "source": [
    "df_posts = execute_pipeline(df_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entro execute pipeline\n",
      "Paso 1\n",
      "Paso 2\n"
     ]
    }
   ],
   "source": [
    "df_comments = execute_pipeline(df_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
